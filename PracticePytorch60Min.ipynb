{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/rashijain/opt/anaconda3/envs/genai_env/lib/python3.12/site-packages (2.2.2)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.2-cp312-cp312-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /Users/rashijain/opt/anaconda3/envs/genai_env/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rashijain/opt/anaconda3/envs/genai_env/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/rashijain/opt/anaconda3/envs/genai_env/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/rashijain/opt/anaconda3/envs/genai_env/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/rashijain/opt/anaconda3/envs/genai_env/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/rashijain/opt/anaconda3/envs/genai_env/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: numpy in /Users/rashijain/opt/anaconda3/envs/genai_env/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/rashijain/opt/anaconda3/envs/genai_env/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rashijain/opt/anaconda3/envs/genai_env/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rashijain/opt/anaconda3/envs/genai_env/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torchvision-0.17.2-cp312-cp312-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "Successfully installed torchvision-0.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_data=datasets.FashionMNIST(\n",
    "    root=\"Data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    "    \n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "train_dataloader=DataLoader(training_data,batch_size=batch_size,shuffle=True)\n",
    "test_dataloader=DataLoader(test_data,batch_size=batch_size,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(x):\n",
    "    print(\"Type: {}\".format(x.type()))\n",
    "    print(\"Shape/size: {}\".format(x.shape)) \n",
    "    print(\"Values: \\n{}\".format(x))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x15947ec90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x158d2dc40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N,C,H,W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X,y in train_dataloader:\n",
    "    print(f\"Shape of X [N,C,H,W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device=torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **`class NeuralNetwork(nn.Module):`**\n",
    "\n",
    "1. **What it does:**\n",
    "   - This defines a new class called `NeuralNetwork`.\n",
    "   - The class inherits from `nn.Module`, which is part of PyTorch's library.\n",
    "\n",
    "2. **Why it's important:**\n",
    "   - In PyTorch, `nn.Module` is the base class for all neural network models. By inheriting from it, `NeuralNetwork` gains all the functionality required to define, train, and run neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "### **`def __init__(self):`**\n",
    "\n",
    "What is the role of __init__ in this context?\n",
    "\n",
    "Think of __init__ as the constructor that defines the \"blueprint\" of the neural network:\n",
    "\n",
    "It initializes all the components (e.g., Flatten, Linear, ReLU) the model will use.\n",
    "\n",
    "These components remain \"inactive\" until called in the forward method.\n",
    "\n",
    "1. **What it does:**\n",
    "   - This is the constructor method for the `NeuralNetwork` class.\n",
    "   - It initializes the object whenever you create an instance of the `NeuralNetwork` class.\n",
    "\n",
    "2. **Why it's important:**\n",
    "   - This method sets up the neural network by defining its layers, parameters, or any necessary configurations.\n",
    "   - \n",
    "Purpose of __init__:\n",
    "\n",
    "The __init__ method sets up the architecture of the model.\n",
    "Layers like Flatten, Linear, or ReLU are only initialized here, meaning they are defined but not yet applied to any data.\n",
    "---\n",
    "\n",
    "### **`super().__init__()`**\n",
    "\n",
    "1. **What it does:**\n",
    "   - This line calls the `__init__` method of the parent class (`nn.Module`).\n",
    "   - It ensures that the base class (`nn.Module`) is properly initialized before adding any custom functionality.\n",
    "\n",
    "2. **Why it's important:**\n",
    "   - Without this line, the `NeuralNetwork` class might not behave as expected since the parent class (`nn.Module`) needs to handle certain setups like tracking layers and parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### Putting it together:\n",
    "\n",
    "This code defines a neural network class that:\n",
    "1. Inherits PyTorch's `nn.Module` to leverage built-in features like automatic gradient computation and parameter management.\n",
    "2. Initializes the parent class to ensure a proper setup for the custom model.\n",
    "3. Provides a structure for defining the architecture of your neural network, which you would do by adding layers in the `__init__` method and implementing the forward pass in another method (e.g., `forward`).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural_network(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class neural_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.linear_relu_stack=nn.Sequential(\n",
    "            nn.Linear(28*28,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10),\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.flatten(x)\n",
    "        logits=self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "    model=neural_network().to(device)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **What is `flatten` doing?**\n",
    "\n",
    "1. **`nn.Flatten` Explanation:**\n",
    "   - `nn.Flatten()` takes the input tensor (typically with multiple dimensions, like a 2D image) and reshapes it into a 1D vector. This is necessary because fully connected (`nn.Linear`) layers expect a 1D input.\n",
    "   - For example, if the input is an image of shape `(batch_size, 28, 28)` (a grayscale image), `nn.Flatten()` reshapes it into `(batch_size, 28*28)`.\n",
    "\n",
    "2. **Why flattening is important:**\n",
    "   - It allows the image data to flow into a fully connected (dense) layer by converting the grid-like 2D data into a flat 1D format.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **What does `28x28x512` mean?**\n",
    "\n",
    "1. **The components:**\n",
    "   - `28x28`: Represents the dimensions of the input image. A grayscale image with a height and width of 28 pixels each.\n",
    "   - `512`: Represents the number of **neurons (nodes)** in the first hidden layer.\n",
    "\n",
    "2. **Flow of data:**\n",
    "   - **Before flattening:** The image is a 2D grid (28x28) for each sample.\n",
    "   - **After flattening:** The image becomes a 1D vector of size 784 (`28 * 28`), which is the input to the first dense layer.\n",
    "\n",
    "---\n",
    "\n",
    "### **How the layers are structured:**\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - The input to the network is a batch of images of size `(batch_size, 28, 28)`.\n",
    "   - The `flatten` layer converts this to `(batch_size, 784)` so it can be fed into the dense layers.\n",
    "\n",
    "2. **First Fully Connected Layer:**\n",
    "   - `nn.Linear(28*28, 512)` takes the flattened input (size 784) and connects it to 512 nodes in the first hidden layer.\n",
    "   - Each of the 784 input values is connected to each of the 512 neurons, creating **784 * 512 connections**.\n",
    "\n",
    "3. **ReLU Activation:**\n",
    "   - `nn.ReLU()` introduces non-linearity after the first dense layer. It helps the network learn complex patterns by activating only positive outputs and zeroing out negative ones.\n",
    "\n",
    "4. **Second Fully Connected Layer:**\n",
    "   - `nn.Linear(512, 512)` connects the 512 neurons in the first hidden layer to another 512 neurons in the second hidden layer.\n",
    "\n",
    "5. **ReLU Activation (again):**\n",
    "   - Another `nn.ReLU()` is applied to introduce non-linearity.\n",
    "\n",
    "6. **Output Layer:**\n",
    "   - `nn.Linear(512, 10)` connects the 512 neurons in the second hidden layer to 10 output neurons.\n",
    "   - The `10` output neurons represent the **classes** (e.g., digits 0-9 if this is for digit classification).\n",
    "\n",
    "---\n",
    "\n",
    "### **Imagining the Layers:**\n",
    "\n",
    "1. **Layer 1 (Flatten):**\n",
    "   - **Input:** `(28x28 image)` → **Output:** `(784,)` (flattened vector).\n",
    "\n",
    "2. **Layer 2 (First Dense Layer):**\n",
    "   - **Input:** `(784,)` → **Output:** `(512,)`.\n",
    "   - **Visualization:** Think of 512 nodes, each receiving inputs from all 784 pixels.\n",
    "\n",
    "3. **Layer 3 (ReLU):**\n",
    "   - Applies non-linearity to the 512 outputs.\n",
    "\n",
    "4. **Layer 4 (Second Dense Layer):**\n",
    "   - **Input:** `(512,)` → **Output:** `(512,)`.\n",
    "\n",
    "5. **Layer 5 (ReLU):**\n",
    "   - Another non-linear transformation.\n",
    "\n",
    "6. **Layer 6 (Output Layer):**\n",
    "   - **Input:** `(512,)` → **Output:** `(10,)`.\n",
    "   - These 10 outputs typically represent class probabilities for each category (after applying something like `softmax`).\n",
    "\n",
    "---\n",
    "\n",
    "### **Flow of Data:**\n",
    "\n",
    "- The image data starts as a grid (28x28).\n",
    "- It is flattened into a 1D vector (784).\n",
    "- Passes through two hidden layers, each with 512 neurons.\n",
    "- Finally, it is reduced to 10 outputs, representing class scores (logits).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural_network(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = neural_network().to(device)\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.rand(1,28,28,device=device)\n",
    "\n",
    "logits=model(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0041, -0.0547,  0.0245, -0.0414,  0.0081,  0.0433,  0.0917, -0.0217,\n",
       "         -0.0039, -0.1045]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1001, 0.0951, 0.1030, 0.0964, 0.1013, 0.1049, 0.1101, 0.0983, 0.1001,\n",
       "         0.0905]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probab=nn.Softmax(dim=1)(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=pred_probab.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Probab {tensor([5])}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted Probab\", {y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of neural_network(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size=len(dataloader.dataset)\n",
    "    \n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X,y=X.to(device),y.to(device)\n",
    "        \n",
    "        pred=model(X)\n",
    "        \n",
    "        loss=loss_fn(pred,y)\n",
    "        \n",
    "        #backpropagation\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "        if batch%100==0:\n",
    "            loss,current=loss.item(),(batch+1)*len(X)\n",
    "            \n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}      \")\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **1. `optimizer.step()`**\n",
    "\n",
    "#### **What it does:**\n",
    "- Updates the parameters of the model (weights and biases) based on the gradients that were computed during backpropagation.\n",
    "\n",
    "#### **How it works:**\n",
    "- After calling `loss.backward()`, the gradients of the loss with respect to each model parameter are stored in the `grad` attribute of the respective parameter.\n",
    "- When you call `optimizer.step()`, the optimizer uses these gradients to adjust the parameters according to the chosen optimization algorithm (e.g., SGD, Adam).\n",
    "- For example, in stochastic gradient descent (SGD):\n",
    "  \\[\n",
    "  \\text{new\\_param} = \\text{param} - \\text{learning\\_rate} \\times \\text{gradient}\n",
    "  \\]\n",
    "\n",
    "#### **Why it's important:**\n",
    "- Without `optimizer.step()`, the model’s weights wouldn’t be updated, and the training process wouldn’t progress.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. `optimizer.zero_grad()`**\n",
    "\n",
    "#### **What it does:**\n",
    "- Clears the gradients of all model parameters.\n",
    "\n",
    "#### **Why it’s needed:**\n",
    "- Gradients in PyTorch accumulate by default. This means that every time you call `loss.backward()`, the computed gradients are added to the ones already stored in the `grad` attribute of the parameters.\n",
    "- If you don’t clear the gradients using `optimizer.zero_grad()`, the old gradients from the previous step will mix with the new gradients, leading to incorrect parameter updates.\n",
    "\n",
    "#### **When to call it:**\n",
    "- It’s typically called at the start of every training step (before `loss.backward()`).\n",
    "\n",
    "#### **Example of Accumulation Issue:**\n",
    "```python\n",
    "# Without zero_grad\n",
    "for i in range(2):\n",
    "    optimizer.zero_grad() if i == 0 else None  # Clearing only once\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    print(param.grad)  # Gradients accumulate, resulting in incorrect updates\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **How They Work Together in a Training Loop:**\n",
    "\n",
    "Here’s how `optimizer.step()` and `optimizer.zero_grad()` are typically used:\n",
    "\n",
    "```python\n",
    "for batch in dataloader:  # Loop over batches of data\n",
    "    optimizer.zero_grad()          # 1. Clear old gradients\n",
    "    predictions = model(inputs)    # 2. Forward pass\n",
    "    loss = loss_fn(predictions, targets)  # 3. Compute loss\n",
    "    loss.backward()                # 4. Backpropagate to compute gradients\n",
    "    optimizer.step()               # 5. Update model parameters\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.058276 [   64/60000      \n",
      "loss: 1.101940 [ 6464/60000      \n",
      "loss: 1.173568 [12864/60000      \n",
      "loss: 1.093444 [19264/60000      \n",
      "loss: 0.984082 [25664/60000      \n",
      "loss: 0.987985 [32064/60000      \n",
      "loss: 1.016739 [38464/60000      \n",
      "loss: 1.008723 [44864/60000      \n",
      "loss: 1.122202 [51264/60000      \n",
      "loss: 0.900272 [57664/60000      \n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.986220 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.959381 [   64/60000      \n",
      "loss: 0.943933 [ 6464/60000      \n",
      "loss: 0.884231 [12864/60000      \n",
      "loss: 0.977122 [19264/60000      \n",
      "loss: 0.838946 [25664/60000      \n",
      "loss: 0.804766 [32064/60000      \n",
      "loss: 0.905596 [38464/60000      \n",
      "loss: 0.814231 [44864/60000      \n",
      "loss: 0.882775 [51264/60000      \n",
      "loss: 0.924000 [57664/60000      \n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.915468 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.872355 [   64/60000      \n",
      "loss: 1.152720 [ 6464/60000      \n",
      "loss: 0.832544 [12864/60000      \n",
      "loss: 1.129115 [19264/60000      \n",
      "loss: 0.796711 [25664/60000      \n",
      "loss: 0.981067 [32064/60000      \n",
      "loss: 0.890738 [38464/60000      \n",
      "loss: 0.900211 [44864/60000      \n",
      "loss: 1.075036 [51264/60000      \n",
      "loss: 0.821283 [57664/60000      \n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.861283 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.833835 [   64/60000      \n",
      "loss: 0.899177 [ 6464/60000      \n",
      "loss: 0.958452 [12864/60000      \n",
      "loss: 0.936190 [19264/60000      \n",
      "loss: 0.728479 [25664/60000      \n",
      "loss: 0.705815 [32064/60000      \n",
      "loss: 0.835613 [38464/60000      \n",
      "loss: 0.682139 [44864/60000      \n",
      "loss: 0.848928 [51264/60000      \n",
      "loss: 0.680064 [57664/60000      \n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.824038 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.786804 [   64/60000      \n",
      "loss: 0.817933 [ 6464/60000      \n",
      "loss: 0.685283 [12864/60000      \n",
      "loss: 0.791677 [19264/60000      \n",
      "loss: 0.878611 [25664/60000      \n",
      "loss: 0.660866 [32064/60000      \n",
      "loss: 0.757391 [38464/60000      \n",
      "loss: 0.709445 [44864/60000      \n",
      "loss: 0.714881 [51264/60000      \n",
      "loss: 0.674062 [57664/60000      \n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.793022 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved pytorch model\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(),\"model.pth\")\n",
    "print(\"saved pytorch model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = neural_network().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
